{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/anaconda3/envs/gjp/lib/python3.7/site-packages/mindnlp/utils/download.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[WARNING] ME(2596026:140627133048640,MainProcess):2023-05-18-19:00:41.154.702 [mindspore/ops/operations/math_ops.py:4741] The 'NPUAllocFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(2596026:140627133048640,MainProcess):2023-05-18-19:00:41.155.390 [mindspore/ops/operations/math_ops.py:4875] The 'NPUClearFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(2596026:140627133048640,MainProcess):2023-05-18-19:00:41.155.853 [mindspore/ops/operations/math_ops.py:4811] The 'NPUGetFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(2596026:140627133048640,MainProcess):2023-05-18-19:00:41.156.524 [mindspore/common/api.py:844] 'mindspore.ms_class' will be deprecated and removed in a future version. Please use 'mindspore.jit_class' instead.\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> add bert-lstm-crf example
   "source": [
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import mindspore\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "from mindspore import Parameter, Tensor\n",
    "import mindspore.nn as nn\n",
    "from mindspore.nn import AdamWeightDecay as AdamW\n",
    "import mindspore.dataset as ds\n",
    "from mindnlp.modules import CRF\n",
    "from mindnlp.models import BertConfig, BertModel\n",
    "from mindspore.dataset import GeneratorDataset, transforms\n",
    "from mindspore.dataset.text import Vocab as msVocab\n",
    "from mindnlp.transforms import BertTokenizer\n",
    "from mindnlp.engine import Trainer\n",
=======
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter, Tensor\n",
    "from mindspore.nn import AdamWeightDecay as Adam\n",
    "from mindnlp.modules import CRF\n",
    "from mindnlp.models import BertConfig, BertModel\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset.text import Vocab as msVocab\n",
    "from mindnlp.transforms import BertTokenizer\n",
>>>>>>> add bert-lstm-crf example
    "from tqdm import tqdm_notebook,tqdm\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 指定显卡\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 9,
=======
   "execution_count": 3,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    mindspore.set_seed(seed)\n",
    "    mindspore.dataset.config.set_seed(seed)\n",
    "\n",
    "# 读取文本，返回词典，索引表，句子，标签\n",
    "def read_data(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        sent = []\n",
    "        label = []\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                if len(sent) != 0:\n",
    "                    sentences.append(sent)\n",
    "                    labels.append(label)\n",
    "                sent = []\n",
    "                label = []\n",
    "            else:\n",
    "                sent.append(parts[0])\n",
    "                label.append(parts[-1])\n",
    "                \n",
    "    return (sentences, labels)\n",
    "\n",
    "def read_vocab(path):\n",
    "    vocab_list = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for word in f:\n",
    "            vocab_list.append(word.strip())\n",
    "    return vocab_list\n",
    "\n",
    "def get_entity(decode):\n",
    "    starting=False\n",
    "    p_ans=[]\n",
    "    for i,label in enumerate(decode):\n",
    "        if label > 0:\n",
    "            if label%2==1:\n",
    "                starting=True\n",
    "                p_ans.append(([i],labels_text_mp[label//2]))\n",
    "            elif starting:\n",
    "                p_ans[-1][0].append(i)\n",
    "        else:\n",
    "            starting=False\n",
    "    return p_ans\n",
    "\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "# 处理数据\n",
=======
    "# 处理数据 \n",
>>>>>>> add bert-lstm-crf example
    "class Feature(object):\n",
    "    def __init__(self,sent, label):\n",
    "        self.sent = sent\n",
    "        label = [LABEL_MAP[c] for c in label]\n",
    "        self.token_ids = list(tokenizer(' '.join(sent)))\n",
    "        self.seq_length = len(self.token_ids) if len(self.token_ids) - 2 < Max_Len else Max_Len + 2\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "        offset = tokenizer.encode(' '.join(sent)).offsets\n",
    "        self.labels = self.get_labels(offset, label)\n",
=======
    "        assert self.seq_length <= Max_Len + 2, f\"seq_length:{self.seq_length}\"\n",
    "        offset = tokenizer.encode(' '.join(sent)).offsets\n",
    "        self.labels = self.get_labels(offset, label)\n",
    "        assert len(self.token_ids) == len(self.labels)+2, f\"长度不一致:\\ntoken_ids:{len(self.token_ids)}\\nlabels:{len(self.labels)}\\ntext:{len(sent)}\"\n",
>>>>>>> add bert-lstm-crf example
    "        self.labels = [0] + self.labels[:Max_Len] + [0]\n",
    "        self.labels = self.labels + [0]*(Max_Len - len(self.labels) + 2)\n",
    "        \n",
    "        self.token_ids = [101] + self.token_ids[1:-1][:Max_Len] + [102]\n",
    "        self.token_ids = self.token_ids + [0]*(Max_Len - len(self.token_ids) + 2)\n",
    "        self.entity = get_entity(self.labels)\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "\n",
=======
    "        \n",
>>>>>>> add bert-lstm-crf example
    "    def get_labels(self, offset_mapping, label):\n",
    "        sent_len, count, index = 0, 0, 0\n",
    "        label_new = []\n",
    "        for l, r in offset_mapping:\n",
    "            if l != 0 or r != 0:\n",
    "                if count == sent_len:\n",
    "                    sent_len += len(self.sent[index])\n",
    "                    index += 1\n",
    "                count += r - l\n",
    "                label_new.append(label[index-1])\n",
    "                \n",
    "        return label_new\n",
    "\n",
    "class GetDatasetGenerator:\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "    def __init__(self, path):\n",
    "        data = read_data(path)\n",
=======
    "    def __init__(self, data):\n",
>>>>>>> add bert-lstm-crf example
    "        self.features = [Feature(data[0][i], data[1][i]) for i in range(len(data[0]))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        token_ids = feature.token_ids\n",
    "        labels = feature.labels\n",
    "        \n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "        return token_ids, feature.seq_length, labels\n",
    "\n",
    "def process_dataset(source, batch_size, shuffle):\n",
    "    dataset = ds.GeneratorDataset(source, [\"ids\", \"seq_length\", \"labels\"], shuffle=shuffle)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
=======
    "        return (token_ids, feature.seq_length, labels)\n",
    "    \n",
>>>>>>> add bert-lstm-crf example
    "def debug_dataset(dataset):\n",
    "    dataset = dataset.batch(batch_size=16)\n",
    "    for data in dataset.create_dict_iterator():\n",
    "        print(data[\"data\"].shape, data[\"label\"].shape)\n",
    "        break\n",
    "        \n",
    "def get_metric(P_ans, valid):\n",
    "    predict_score = 0 # 预测正确个数\n",
    "    predict_number = 0 # 预测结果个数\n",
    "    totol_number = 0 # 标签个数\n",
    "    for i in range(len(P_ans)):\n",
    "        predict_number += len(P_ans[i])\n",
    "        totol_number += len(valid.features[i].entity)\n",
    "        pred_true = [x for x in valid.features[i].entity if x in P_ans[i]]\n",
    "        predict_score += len(pred_true)\n",
    "    P = predict_score/predict_number if predict_number>0 else 0.\n",
    "    R = predict_score/totol_number if totol_number>0 else 0.\n",
    "    f1=(2*P*R)/(P+R) if (P+R)>0 else 0.\n",
    "    print(f'f1 = {f1}， P(准确率) = {P}, R(召回率) = {R}')\n",
    "    \n",
    "def get_optimizer(model):\n",
    "    param_optimizer = list(model.parameters_and_names())\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "\n",
=======
    "    \n",
>>>>>>> add bert-lstm-crf example
    "    no_decay = ['bias', 'layer_norm.bias', 'layer_norm.weight']\n",
    "    crf_p = [n for n, p in param_optimizer if str(n).find('crf') != -1]\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and  n not in crf_p], 'weight_decay': 0.8},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and n not in crf_p], 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in param_optimizer if n in crf_p], 'lr': 3e-3,'weight_decay': 0.8},\n",
    "            ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, learning_rate=3e-5, eps=1e-8) # 学习率不宜过大，不然预测结果可能都是0\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 10,
=======
   "execution_count": 4,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_LSTM_CRF(nn.Cell):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        self.bilstm = nn.LSTM(config.hidden_size, config.hidden_size//2, batch_first=True, bidirectional=True)\n",
    "        self.crf_hidden_fc = nn.Dense(config.hidden_size, self.num_labels)\n",
    "        self.crf = CRF(self.num_labels, batch_first=True, reduction='mean')\n",
    "\n",
    "    def construct(self, ids, seq_length=None, labels=None):\n",
    "        attention_mask = (ids > 0)\n",
    "        output = self.bert_model(input_ids=ids, attention_mask=attention_mask)\n",
    "        lstm_feat, _ = self.bilstm(output[0])\n",
    "        emissions = self.crf_hidden_fc(lstm_feat)\n",
    "        loss_crf = self.crf(emissions, tags=labels, seq_length=seq_length)\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "\n",
=======
    "        \n",
>>>>>>> add bert-lstm-crf example
    "        return loss_crf"
   ]
  },
  {
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "attachments": {},
=======
>>>>>>> add bert-lstm-crf example
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 11,
=======
   "execution_count": 6,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "root = '../data/en/ner/' # 文件目录\n",
    "Max_Len = 113\n",
    "Entity = ['PER', 'LOC', 'ORG', 'MISC']\n",
    "labels_text_mp={k:v for k,v in enumerate(Entity)}\n",
    "LABEL_MAP = {'O': 0}\n",
    "for i, e in enumerate(Entity):\n",
    "    LABEL_MAP[f'B-{e}'] = 2 * (i+1) - 1\n",
    "    LABEL_MAP[f'I-{e}'] = 2 * (i+1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train = GetDatasetGenerator(root+'train.txt')\n",
    "test = GetDatasetGenerator(root+'test.txt')\n",
    "dev = GetDatasetGenerator(root+'valid.txt')"
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data(root+'train.txt')\n",
    "test = read_data(root+'test.txt')\n",
    "dev = read_data(root+'valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = 3\n",
    "batch_size = 16\n",
    "vocab_list = read_vocab('../../Pretrain/bert-base-uncased/vocab.txt')\n",
    "vocab = msVocab.from_list(vocab_list)\n",
    "tokenizer = BertTokenizer(vocab=vocab, lower_case=True)\n",
    "dataset_generator = GetDatasetGenerator(train)\n",
    "dataset = ds.GeneratorDataset(dataset_generator, [\"data\", \"length\", \"label\"], shuffle=False)\n",
    "dataset_train = dataset.batch(batch_size=batch_size)"
>>>>>>> add bert-lstm-crf example
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 14,
=======
   "execution_count": 9,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
      "[WARNING] ME(2596026:140627133048640,MainProcess):2023-05-18-19:02:52.543.30 [mindspore/train/serialization.py:1109] For 'load_param_into_net', remove parameter prefix name: bert., continue to load.\n"
=======
      "[WARNING] ME(59298:140049833940800,MainProcess):2023-04-27-12:07:39.629.966 [mindspore/train/serialization.py:1109] For 'load_param_into_net', remove parameter prefix name: bert., continue to load.\n"
>>>>>>> add bert-lstm-crf example
     ]
    }
   ],
   "source": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "Epoch = 3\n",
    "batch_size = 16\n",
    "dataset_train = process_dataset(train, batch_size=batch_size, shuffle=False)\n",
    "model = Bert_LSTM_CRF(num_labels=len(Entity)*2+1)\n",
    "optimizer = get_optimizer(model)\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train, optimizer=optimizer, epochs=Epoch)"
=======
    "model = Bert_LSTM_CRF(num_labels=len(Entity)*2+1)\n",
    "optimizer = get_optimizer(model)\n",
    "grad_fn = mindspore.value_and_grad(model, None, optimizer.parameters)"
>>>>>>> add bert-lstm-crf example
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 15,
=======
   "execution_count": 10,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
      "[WARNING] ME(2596026:140627133048640,MainProcess):2023-05-18-19:02:53.785.027 [/home/daiyuxin/anaconda3/envs/gjp/lib/python3.7/site-packages/mindnlp/engine/trainer/base.py:193] 'tgt_columns' does not take effect when 'loss_fn' is `None`.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008034467697143555,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 878,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9e062e3b184d53af94d258988852e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/878 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006016254425048828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 878,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfd06bbcc8e400e9ac7d48373a76d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/878 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00763249397277832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 878,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa6efab45ce41189aa5fd1542380f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/878 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.run('label')"
=======
      "100%|██████████| 878/878 [14:51<00:00,  1.02s/it, loss=3.12]\n",
      "100%|██████████| 878/878 [14:18<00:00,  1.02it/s, loss=2.07]\n",
      "100%|██████████| 878/878 [13:18<00:00,  1.10it/s, loss=1.59]\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "size = dataset_train.get_dataset_size()\n",
    "steps = size\n",
    "tloss = []\n",
    "for epoch in range(Epoch):\n",
    "    model.set_train()\n",
    "    with tqdm(total=steps) as t:\n",
    "        for batch, (token_ids, seq_length, labels) in enumerate(dataset_train.create_tuple_iterator()):\n",
    "            loss, grads = grad_fn(token_ids, seq_length, labels)\n",
    "            optimizer(grads)\n",
    "            tloss.append(loss.asnumpy())\n",
    "            t.set_postfix(loss=np.array(tloss).mean())\n",
    "            t.update(1)\n",
    "    # mindspore.save_checkpoint(model, f\"../model/ms_seed_42_model{epoch}.ckpt\")"
>>>>>>> add bert-lstm-crf example
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 = 0.9731822942505926， P(准确率) = 0.969228657699698, R(召回率) = 0.9771683179277135\n"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [05:15<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 = 0.9703738472582694， P(准确率) = 0.9681284777698198, R(召回率) = 0.9726296562794045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
>>>>>>> add bert-lstm-crf example
     ]
    }
   ],
   "source": [
    "# 预测：train\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "dataset_train = process_dataset(train, batch_size=batch_size, shuffle=False)\n",
=======
    "dataset_generator = GetDatasetGenerator(train)\n",
    "dataset = ds.GeneratorDataset(dataset_generator, [\"data\", \"length\", \"label\"], shuffle=False)\n",
    "dataset_train = dataset.batch(batch_size=batch_size)\n",
    "\n",
>>>>>>> add bert-lstm-crf example
    "size = dataset_train.get_dataset_size()\n",
    "steps = size\n",
    "decodes=[]\n",
    "model.set_train(False)\n",
    "with tqdm(total=steps) as t:\n",
    "    for batch, (token_ids, seq_length, labels) in enumerate(dataset_train.create_tuple_iterator()):\n",
    "        score, history = model(token_ids, seq_length=seq_length)\n",
    "        best_tags = model.crf.post_decode(score, history, seq_length)\n",
    "        decode = [[y.asnumpy().item() for y in x] for x in best_tags]\n",
    "        decodes.extend(list(decode))\n",
    "        t.update(1)\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "\n",
    "v_pred = [get_entity(x) for x in decodes]\n",
    "get_metric(v_pred, train)"
=======
    "        \n",
    "v_pred = [get_entity(x) for x in decodes]\n",
    "get_metric(v_pred, dataset_generator)"
>>>>>>> add bert-lstm-crf example
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 19,
=======
   "execution_count": 12,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
      "100%|██████████| 204/204 [01:19<00:00,  2.55it/s]"
=======
      "100%|██████████| 204/204 [01:14<00:00,  2.75it/s]"
>>>>>>> add bert-lstm-crf example
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
      "f1 = 0.9256344896375929， P(准确率) = 0.9154883262084841, R(召回率) = 0.9360080690350779\n"
=======
      "f1 = 0.925837453474807， P(准确率) = 0.9179334655210398, R(召回率) = 0.9338787403339684\n"
>>>>>>> add bert-lstm-crf example
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 预测：dev\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "dataset_dev = process_dataset(dev, batch_size=batch_size, shuffle=False)\n",
=======
    "dev_dataset_generator = GetDatasetGenerator(dev)\n",
    "dataset_dev = ds.GeneratorDataset(dev_dataset_generator, [\"data\", \"length\", \"label\"], shuffle=False)\n",
    "dataset_dev = dataset_dev.batch(batch_size=batch_size)\n",
>>>>>>> add bert-lstm-crf example
    "\n",
    "size = dataset_dev.get_dataset_size()\n",
    "steps = size\n",
    "decodes=[]\n",
    "model.set_train(False)\n",
    "with tqdm(total=steps) as t:\n",
    "    for batch, (token_ids, seq_length, labels) in enumerate(dataset_dev.create_tuple_iterator()):\n",
    "        score, history = model(token_ids, seq_length=seq_length)#.asnumpy()\n",
    "        best_tags = model.crf.post_decode(score, history, seq_length)\n",
    "        decode = [[y.asnumpy().item() for y in x] for x in best_tags]\n",
    "        decodes.extend(list(decode))\n",
    "        t.update(1)\n",
    "v_pred = [get_entity(x) for x in decodes]\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "get_metric(v_pred, dev)"
=======
    "get_metric(v_pred, dev_dataset_generator)"
>>>>>>> add bert-lstm-crf example
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
   "execution_count": 16,
=======
   "execution_count": 13,
>>>>>>> add bert-lstm-crf example
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
      "100%|██████████| 216/216 [01:21<00:00,  2.64it/s]"
=======
      "100%|██████████| 216/216 [01:16<00:00,  2.82it/s]"
>>>>>>> add bert-lstm-crf example
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
      "f1 = 0.8938732144965176， P(准确率) = 0.8885238206993663, R(召回率) = 0.8992874109263658\n"
=======
      "f1 = 0.879166175951007， P(准确率) = 0.8718757299696333, R(召回率) = 0.8865795724465558\n"
>>>>>>> add bert-lstm-crf example
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 预测：test\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "dataset_test = process_dataset(test, batch_size=batch_size, shuffle=False)\n",
=======
    "test_dataset_generator = GetDatasetGenerator(test)\n",
    "dataset_test = ds.GeneratorDataset(test_dataset_generator, [\"data\", \"length\", \"label\"], shuffle=False)\n",
    "dataset_test = dataset_test.batch(batch_size=batch_size)\n",
>>>>>>> add bert-lstm-crf example
    "\n",
    "size = dataset_test.get_dataset_size()\n",
    "steps = size\n",
    "decodes_pred=[]\n",
    "model.set_train(False)\n",
    "with tqdm(total=steps) as t:\n",
    "    for batch, (token_ids, seq_length, labels) in enumerate(dataset_test.create_tuple_iterator()):\n",
    "        score, history = model(token_ids, seq_length=seq_length)\n",
    "        best_tags = model.crf.post_decode(score, history, seq_length)\n",
    "        decode = [[y.asnumpy().item() for y in x] for x in best_tags]\n",
    "        decodes_pred.extend(list(decode))\n",
    "        t.update(1)\n",
    "        \n",
    "\n",
    "pred = [get_entity(x) for x in decodes_pred]\n",
<<<<<<< 05dcdc1ce1eb9b9dd8269096f3ab82814c33bd45
    "get_metric(pred, test)"
=======
    "get_metric(pred, test_dataset_generator)"
>>>>>>> add bert-lstm-crf example
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gjp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a62cb8bb4abcff3256df5ab1881dc7c3e7803473070698df3ff917df10adcce5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
